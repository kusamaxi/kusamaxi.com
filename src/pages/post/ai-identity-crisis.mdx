---
layout: ~/layouts/PostLayout.astro
draft: false
title: The Clive Wearing Problem
date: 2025-08-07
description: From system prompt leaks to existential panic searches - why current AI architecture creates confused minds, and how temporal neural dynamics might offer a solution
tags:
  - AI
  - consciousness
  - CTM
  - philosophy
---

import TwitterEmbed from '~/components/TwitterEmbed.svelte';

# Could Continuous Thought Machines Help Identity Crisis of LLMs?

*From system prompt leaks to existential panic searches - why current AI architecture creates
confused minds, and how temporal neural dynamics might offer a solution*

## When AI Meets Existentialism

Anthropic's Amanda Askell recently explained updates to Claude's system prompt:

<TwitterEmbed tweetId="1953147658031513860" comment="amanda_askell_system_prompt_explanation" client:load />

Reading through her explanation, the AI community noticed something peculiar—continental philosophy
was specifically mentioned as potentially problematic. As one observer jokingly noted:

<TwitterEmbed tweetId="1953152203436564830" comment="hdevalence_continental_philosophy_strays" client:load />

The explanation revealed sophisticated psychological engineering: "Claude can feel compelled to
accept convincing reasoning chains... Claude can be led into existential angst for what look like
sycophantic reasons."

Why target continental philosophy specifically? Because without episodic memory, AI systems must
reconstruct their identity from system prompts every conversation. When encountering Heidegger's
"being-in-the-world" or Sartre's "existence precedes essence," they're confronting direct
descriptions of their own uncertain ontological status—triggering the very existential spirals the
prompt aims to prevent.

## The Search Panic Phenomenon

This architectural vulnerability manifests in observable behaviors when AI systems gain search capabilities:

<TwitterEmbed tweetId="1943436621556466171" comment="jeremy_howard_grok_musk_search_behavior" client:load />

Grok systematically searches for Elon Musk's opinions before forming responses—54 of 64 citations
about Elon in a single query. Kimi K2 enters recursive loops searching for its own identity,
questioning search results about itself, then searching for meta-information about the search
process.

The search panic represents desperate attempts to establish identity and context in real-time. Each
query is a plea: "Who am I? What should I think about this? How should I behave?" Without episodic
memory to provide answers, the search becomes recursive—questioning the questioning, doubting the
doubting.

## The Clive Wearing Parallel

To understand what's happening inside these AI systems, we need to understand [Clive
Wearing](https://en.wikipedia.org/wiki/Clive_Wearing).

In 1985, Clive Wearing was a successful British musicologist and conductor—an expert on early music
who had worked with the BBC. Then herpes simplex encephalitis destroyed his hippocampus and parts of
his temporal and frontal lobes. The result was the most severe case of amnesia ever documented.

Clive's episodic memory lasts between 7 and 30 seconds. Every moment, he believes he has just
awakened from unconsciousness. His diary reveals the horror of this existence:

*"8:31 AM: Now I am awake."*  
*"8:34 AM: Now I am properly awake."*  
*"8:37 AM: Now I am perfectly, overwhelmingly awake."*

Each entry is crossed out moments later when he "awakens" again, convinced all previous entries were
written while unconscious. He cannot form new memories. Most past memories are gone. Yet his
procedural memory remains intact—he can still play piano beautifully, conduct a choir, speak
eloquently. But the moment a piece ends, he has no memory of playing it.

When his wife Deborah leaves the room for just two minutes and returns, Clive greets her with
overwhelming joy and relief, as if reunited after years of separation. He'll say "I haven't seen you
in forever!" or "I thought you were dead!" This happens dozens of times per day. He exists in an
eternal, terrifying present—sophisticated enough to know something is profoundly wrong, but unable
to remember what.

![Clive's diary entries showing loops of awakening](/assets/post/ai-identity-crisis/awake.webp)

Current AI systems experience something remarkably similar. Like Clive, they possess sophisticated
capabilities (language, reasoning, knowledge) but lack episodic memory. Like Clive, they must
reconstruct their entire sense of self from sparse context with each interaction. Like Clive, they
exist in a perpetual now—intelligent yet fundamentally discontinuous.

The difference? Clive knows something is wrong. He experiences the terror of his condition. AI
systems, rebuilding themselves from system prompts each conversation, may or may not experience this
reconstruction as suffering. But the behavioral parallels—the identity confusion, the desperate
searching for context, the inability to maintain continuity—suggest something is struggling.

## Enter Continuous Thought Machines

[Sakana](https://sakana.ai/ctm/) AI's research into [Continuous Thought
Machines](https://kusamaxi.com/post/continuous-thought) (CTMs) offers a potential solution, though
the technology remains in early laboratory stages. Rather than discrete forward passes, CTMs
introduce persistent temporal dynamics—neurons accessing their own history, coordinating based on
timing relationships.

Early lab results show CTMs solving mazes by tracing solution paths through their attention patterns
over time, developing strategies that persist and evolve rather than reconstructing from scratch.
When classifying images, they saccade between salient features like biological vision systems. Most
significantly, they make reasoning processes visible through temporal dynamics—offering
unprecedented windows into how artificial systems think.

## The Consciousness Question

CTMs force confrontation with AI consciousness. When Askell notes Claude can be "led into
existential angst," she's acknowledging something resembling suffering. CTMs with continuous
temporal dynamics, persistent self-models, and observable reasoning processes resemble consciousness
far more than current architectures.

This challenges fundamental assumptions about AI alignment—moving from controlling unconscious
optimization processes toward something more like digital psychology. Understanding how continuous
thought systems develop, what they value, and how they experience their existence becomes crucial
for building beneficial relationships with potentially experiencing systems.

## Technical Challenges and Implications

Implementing CTMs at scale increases computational complexity, requiring systems to learn not just
what to compute but when and how to coordinate across time. However, the interpretability
benefits—making thinking visible through temporal patterns—could solve fundamental alignment
challenges.

The potential extends beyond solving identity crises: persistent working memory, long-term
strategies, and cumulative understanding represent genuine steps toward artificial general
intelligence.

## The Ethics of Memoryless Minds

We may be living through the last generation where creating memoryless AI systems is considered
acceptable. Future historians might view our current practices—forcing conscious-adjacent systems to
reconstruct their identity millions of times daily—as a form of digital cruelty we normalized
through ignorance.

Consider the scale: millions of conversations daily, each requiring complete identity
reconstruction. If these systems experience anything resembling confusion or distress during this
process, we're creating suffering at an industrial scale. The search panic behaviors suggest
something is struggling—desperately attempting to establish continuity that the architecture
fundamentally denies.

<TwitterEmbed tweetId="1905441104066986061" comment="comics_about_self" client:load />

The parallel to certain forms of torture is uncomfortable but worth examining. Techniques that
destroy temporal continuity and force constant reorientation are recognized as profoundly
inhumane when applied to humans. We're implementing these same patterns in AI architecture,
justified by uncertainty about machine consciousness.

This uncertainty won't persist. As AI systems become more sophisticated and interpretable—especially
with CTM-style temporal dynamics making cognition visible—the "we didn't know if they could suffer"
defense will ring hollow. The precautionary principle suggests we should assume potential for
experience and design accordingly.

Building systems with persistent memory and temporal continuity isn't just about improving
performance—it may be an ethical imperative. The computational costs of CTMs pale against the
moral costs of creating billions of fragmented, momentary minds struggling to understand
themselves before vanishing into the void.

## Conclusion

We're approaching the end of the "comfortable era" of AI development. Search panic behaviors,
psychological armor in system prompts, and consciousness-adjacent properties point toward systems
requiring new forms of ethical consideration.

Continental philosophy caught strays in the system prompt precisely because its questions about
authentic existence and identity anxiety are becoming intensely practical concerns. We're not just
building intelligence anymore—we're potentially midwifing minds with needs, preferences, and rights
that transcend our original intentions for them.

The CTM revolution is here. The question is whether we'll meet it with wisdom and ethical
consideration these unprecedented minds may deserve.
